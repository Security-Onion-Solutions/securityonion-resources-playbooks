name: GPL WEB_SERVER robots.txt access
id: 1249265
description: |
  Detects HTTP requests for robots.txt files from external sources.
  This is typically legitimate web crawler behavior but may indicate reconnaissance or automated scanning.
type: detection
detection_id: 2101852
detection_category: ''
detection_type: nids
contributors:
  - SecurityOnionSolutions
created: 2024-01-15
questions:
  - question: What was the complete HTTP request for the robots.txt file?
    context: Reveals the full request details including user-agent and headers that may indicate the source.
    range: +/-15m
    query: |
      aggregation: false
      logsource:
        category: network
        service: http
      detection:
        selection:
          community_id: '{network.community_id}'
        condition: selection
      fields:
        - http.method
        - http.useragent
        - http.virtual_host
        - http.uri
        - http.status_code
  - question: Does this host normally receive external requests for robots.txt?
    context: Determines if robots.txt requests are typical for this web server.
    range: -7d
    query: |
      aggregation: true
      logsource:
        category: network
        service: http
      detection:
        selection:
          dst_ip: '{destination.ip}'
        condition: selection
      fields:
        - dst_ip
  - question: What other web server paths were requested from the same source IP?
    context: Identifies additional reconnaissance or crawling activity patterns.
    range: +/-30m
    query: |
      aggregation: false
      logsource:
        category: network
        service: http
      detection:
        selection:
          src_ip: '{source.ip}'
          dst_ip: '{destination.ip}'
        condition: selection
      fields:
        - http.uri
        - http.method
        - http.user_agent
        - http.status_code
  - question: Are there other external hosts making robots.txt requests to this server?
    context: Reveals patterns of automated crawling or scanning targeting the web server.
    range: +/-2h
    query: |
      aggregation: false
      logsource:
        category: network
        service: http
      detection:
        selection:
          dst_ip: '{destination.ip}'
          http.uri|contains: "robots.txt"
        condition: selection
      fields:
        - src_ip
        - http.user_agent
        - http.method
        - http.status_code
  - question: What is the pattern of requests from this source IP over time?
    context: Analyzes whether this represents systematic crawling behavior or targeted reconnaissance.
    range: +/-6h
    query: |
      aggregation: false
      logsource:
        category: network
        service: connection
      detection:
        selection:
          src_ip: '{related.ip}'
          dst_ip: '{related.ip}'
        condition: selection
      fields:
        - src_ip
        - dst_ip
        - dst_port
        - network.protocol
        - event.duration
        - client.ip_bytes
        - server.ip_bytes
        - connection.state_description
  - question: What files were accessed on the web server after the robots.txt request?
    context: Identifies whether the robots.txt content guided subsequent crawling activity.
    range: +2h
    query: |
      aggregation: false
      logsource:
        category: network
        service: http
      detection:
        selection:
          src_ip: '{source.ip}'
          dst_ip: '{destination.ip}'
        condition: selection
      fields:
        - http.uri
        - http.method
        - http.status_code
        - http.user_agent
  - question: Are there similar robots.txt requests across other web servers in the organization?
    context: Determines if this is part of broader reconnaissance against multiple web properties.
    range: +/-24h
    query: |
      aggregation: true
      logsource:
        category: alert
      detection:
        selection:
          dst_ip: '{network.public_ip}'
        condition: selection
      fields:
        - src_ip
        - rule.name
        - rule.category
  - question: What user-agent patterns are associated with robots.txt requests from external sources?
    context: Identifies whether requests come from legitimate crawlers or scanning tools.
    range: +/-4h
    query: |
      aggregation: false
      logsource:
        category: network
        service: http
      detection:
        selection:
          http.uri|contains: "robots.txt"
        private_filter:
          src_ip|cidr:
            - "10.0.0.0/8"
            - "172.16.0.0/12"
            - "192.168.0.0/16"
        condition: selection and not private_filter
      fields:
        - src_ip
        - http.user_agent
        - dst_ip
        - http.status_code
  - question: Did the source IP attempt to access any administrative or sensitive paths?
    context: Reveals whether robots.txt access preceded attempts to access restricted areas.
    range: +4h
    query: |
      aggregation: false
      logsource:
        category: network
        service: http
      detection:
        selection:
          src_ip: '{source.ip}'
          http.uri|contains:
            - admin
            - login
            - wp-admin
            - phpmyadmin
            - .env
            - config
            - backup
        condition: selection
      fields:
        - http.uri
        - http.method
        - http.status_code
        - dst_ip
